{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d16282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msparknlp\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43msparknlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\sparknlp\\__init__.py:273\u001B[0m, in \u001B[0;36mstart\u001B[1;34m(gpu, spark23, spark24, spark32, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\u001B[0m\n\u001B[0;32m    271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m SparkRealTimeOutput()\n\u001B[0;32m    272\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 273\u001B[0m     spark_session \u001B[38;5;241m=\u001B[39m \u001B[43mstart_without_realtime_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m spark_session\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\sparknlp\\__init__.py:177\u001B[0m, in \u001B[0;36mstart.<locals>.start_without_realtime_output\u001B[1;34m()\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cluster_tmp_dir \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    175\u001B[0m     builder\u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.jsl.settings.storage.cluster_tmp_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m, cluster_tmp_dir)\n\u001B[1;32m--> 177\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    226\u001B[0m         sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m     sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m    231\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc)\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\context.py:384\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 384\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\context.py:144\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    141\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 144\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0;32m    147\u001B[0m                   conf, jsc, profiler_cls)\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\context.py:331\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[1;32m--> 331\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    332\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    105\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[1;32m--> 108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJava gateway process exited before sending its port number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[0;32m    111\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[1;31mException\u001B[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a152296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\JetBrains\\PyCharm 2021.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_utils.py\", line 69, in attach_to_debugger\n",
      "    debugger.prepare_to_run(enable_tracing_from_start=False)\n",
      "TypeError: prepare_to_run() got an unexpected keyword argument 'enable_tracing_from_start'\n",
      "Failed to connect to target debugger.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# pydev_debug_cell\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msparknlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpretrained\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PretrainedPipeline\n\u001B[0;32m      4\u001B[0m explain_document_pipeline \u001B[38;5;241m=\u001B[39m PretrainedPipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexplain_document_ml\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m annotations \u001B[38;5;241m=\u001B[39m explain_document_pipeline\u001B[38;5;241m.\u001B[39mannotate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe are very happy about SparkNLP\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\sparknlp\\__init__.py:19\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mthreading\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msparknlp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m annotator\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msparknlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DocumentAssembler, Finisher, EmbeddingsFinisher, TokenAssembler, Chunk2Doc, Doc2Chunk\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkConf\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\sparknlp\\annotator.py:21\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Module containing all available Annotators of Spark NLP and their base\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03mclasses.\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msparknlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Do NOT delete. Looks redundant but this is key work around for python 2 support.\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mversion_info[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\sparknlp\\common.py:18\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#  Copyright 2017-2022 John Snow Labs\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#  Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#  See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m#  limitations under the License.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Contains Properties for the Annotator classes.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JavaMLWritable\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwrapper\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JavaModel, JavaEstimator\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparam\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mshared\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Param, TypeConverters\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\ml\\__init__.py:22\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one or more\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# contributor license agreements.  See the NOTICE file distributed with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;124;03mDataFrame-based machine learning APIs to let users quickly assemble and configure practical\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;124;03mmachine learning pipelines.\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Estimator, Model, Predictor, PredictionModel, \\\n\u001B[0;32m     23\u001B[0m     Transformer, UnaryTransformer\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline, PipelineModel\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m classification, clustering, evaluation, feature, fpm, \\\n\u001B[0;32m     26\u001B[0m     image, recommendation, regression, stat, tuning, util, linalg, param\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\ml\\base.py:25\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m since\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inherit_doc\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparam\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mshared\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n\u001B[0;32m     26\u001B[0m     HasPredictionCol, Params\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m udf\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StructField, StructType\n",
      "File \u001B[1;32m~\\.conda\\envs\\sparknlp\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py:21\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mabc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ABCMeta\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjava_gateway\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JavaObject\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DenseVector, Vector, Matrix\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "explain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")\n",
    "annotations = explain_document_pipeline.annotate(\"We are very happy about SparkNLP\")\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}